---
title: "Script DADA2 for for unpaired reads"
author: "Denise Ong"
date: "9/2/2021"
output: html_document
---

This script is adapted from "script_dada2.R" by Daniel. I copied over the sections to process unpaired reads and annotated some parts. This is just for the R1 reads from the 2X250 petB run to get preliminary data. To input the parameters and the directories, change the following:
Parameters:
 -" param_dada2_datasetD1.1"

For the directories:
- "directories_DADA2_datasetD1.1.R" -- for C14 sorted syn cells R1
- "directories_DADA2_datasetD1.2.R" -- for CTD petB nested R1
- "directories_DADA2_datasetD1.3.R" -- for CTD petB normal R1
- "directories_DADA2_datasetD1.4.R" -- for C14 sorted syn cells R2
- "directories_DADA2_datasetD1.5.R" -- for CTD petB nested R2
- "directories_DADA2_datasetD1.6.R" -- for CTD petB normal R2
I also wrote the script at the end to export as phyloseq file.

Note: careful to run because there are some options for R1 vs R2 reads.
- For cutadapt for primers
- For assigning taxonomy

# Preparation
## Libraries
```{r}
suppressPackageStartupMessages({
    library(dada2) # Must use version >= 1.12
    library(Biostrings)
    library(ShortRead)
    library(stringr)
    library(ggplot2)
    library(dplyr)
    library(tidyr)
    library(tibble)
    library(readr)
    library(purrr)
    
    library("optparse")
})
```

## Parameters
```{r results='hide'}
## First read in the arguments listed at the command line
option_list = list(
  make_option(c("-d", "--dataset"), type="character", action = "store", default=006, 
              help="ID of dataset to process [default= %default]", metavar="number"),
  make_option(c("-t", "--test"), type="character", action = "store_true", default=FALSE, 
              help="Test [default= %default]", metavar="logical")
) 

opt_parser = OptionParser(option_list=option_list)
opt = parse_args(opt_parser)

dataset_id  = opt$dataset
#testing = opt$test

## Read the parameter file and print it in output file  

file_param <- str_c("process_DADA2/param_dada2_datasetD1.1.R") # Change this to the parameter source code

source(file_param)

#system(str_c("cat ",file_param))

## When testing is on, disable dada2 and below  

#if (testing) {
#  do_dada2    <- FALSE
#  do_taxo <- FALSE
#  bigdata <- FALSE
#  multithread <- FALSE
#  multithread_filter <- FALSE  
#}

## Take care of novel parameters 

if(!exists("multithread_filter")) multithread_filter <- FALSE # To prevent problem with SLURM
if(!exists("remove_primers") & do_cutadapt == TRUE) remove_primers <- TRUE # For older param versions that did not contain remove_primers


#print(sessionInfo())
```

## Define variables
```{r}
# Maximum number of quality plots to print
max_plot_quality = 6

# For assigning taxonomy by chunks
taxo_slice_size = 1000

# Primer information
primer_length_fwd <- str_length(FWD)
primer_length_rev <- str_length(REV)
```

## Define directories
```{r} 
source("process_DADA2/directories_DADA2_datasetD1.6.R")
```

## Get file names
```{r}
fns <- sort(list.files(dir_fastq, full.names = TRUE))
fns <- fns[str_detect( basename(fns),file_identifier)]

fns_R2 <- fns[str_detect( basename(fns),R2_identifier)]
fns <- fns_R2

  fns.fastq <- fns
  fns.filtN <- str_c(dir_fastqN, basename(fns))
  fns.cut <- str_c(dir_cutadapt, basename(fns))
  fns.filt <- str_c(dir_filtered, basename(fns))
  #print(fns)
  sample.names <- str_split(basename(fns), pattern = file_name_separator, simplify = TRUE)

  # Extract sample names, assuming filenames have format: SAMPLENAME_XXX.fastq
sample.names <- str_split(basename(fns_R2), pattern = file_name_separator, simplify = TRUE)
sample.names <- sample.names[,1]
sample.names # check if sample.names are generated
sample_names_check <- data.frame(sample.names) #Generate dataframe to compare againt the sample list.

fns.summary <- fns
 summary <- data.frame()
  for(i in 1:length(fns.summary)) {
    # For the next line to work needs to install the latest Biostrings lib (see https://github.com/benjjneb/dada2/issues/766)
    geom <- fastq.geometry(fns.summary[i])
    summary_one_row <- data.frame (n_seq=geom[1], file_name=basename(fns.summary[i]))
    summary <- bind_rows(summary, summary_one_row)
    print(paste("Finished with file:",i ,fns.summary[i], summary_one_row$n_seq,"sequences", sep=" "))
  }
  
write_tsv(summary, output_path( "_summary_raw_files.tsv"))

ggplot(summary, aes(x = n_seq)) + geom_histogram(alpha = 0.5, position = "identity", 
                                            binwidth = 10)

# Plot quality for reads

plotQualityProfile(fns[1:10])
```

## Remove primers
```{r}
#Step 1
 allOrients <- function(primer) {
    
    require(Biostrings)
    dna <- DNAString(primer)  # The Biostrings works w/ DNAString objects rather than character vectors
    orients <- c(Forward = dna, Complement = complement(dna), Reverse = reverse(dna), 
                 RevComp = reverseComplement(dna))
    return(sapply(orients, toString))  # Convert back to character vector
  }
  FWD.orients <- allOrients(FWD)
  REV.orients <- allOrients(REV)
  FWD.orients
  REV.orients
 
#Step 2
out_N <- filterAndTrim(fns.fastq, fns.filtN,  
                          maxN = 0, minQ = -10, multithread = multithread) 

# Step 3
primerHits <- function(primer, fn) {
    # Exist if primer is empty
    if (primer == "") return(0)
    # Counts number of reads in which the primer is found 
    nhits <- vcountPattern(primer, sread(readFastq(fn)), fixed = FALSE)
    return(sum(nhits > 0))
}
primer_test <- rbind(FWD.ForwardReads = sapply(FWD.orients, primerHits, fn = fns.fastq[[1]]),  
                         REV.ForwardReads = sapply(REV.orients, primerHits, fn = fns.fastq[[1]]))
print(primer_test)
```

```{r}
#Step 4: Run cutadapt - This does not work under R studio....

cutadapt <- "/opt/miniconda3/envs/cutadaptenv/bin/cutadapt"
system2(cutadapt, args = "--version")

FWD.RC <- dada2:::rc(FWD)
REV.RC <- dada2:::rc(REV)

#For R1 reads

# Assume all reads to be in the good orientation
# Trim FWD and the reverse-complement of REV off of reads.  The reverse cannot be anchored)
      flags_FWD = ""
      flags_REV = ""
      if (FWD != "") flags_FWD <- str_c("-g ", anchor, FWD) 
      if (REV != "") flags_REV <- str_c("-a ", REV.RC) 
      
#For R2 reads

# Trim REV and the reverse-complement of FWD off of reads.  The reverse cannot be anchored)

flags_FWD = ""
flags_REV = ""
if (FWD != "") flags_FWD <- str_c("-g ", anchor, REV) 
if (REV != "") flags_REV <- str_c("-a", FWD.RC)

for(i in seq_along(fns)) {
        system2(cutadapt, args = c(flags_FWD, 
                                   flags_REV,
                                   "-o",  fns.cut[i],           # output files
                                   fns.fastq[i],                # input files
                                   "--cores=0",                 # automatic detection of number of cores
                                   "--max-n=0",                 # Remove any N
                                   "--trim-n",                  # Remove N at end of reads
                                   "--discard-untrimmed"))      # remove all reads where primer not found   
        # system2(cutadapt, args = c(flags_REV, 
        #                    "-o", fns.cut[i],            # output files
        #                     temp_file))      # Keep the reads even if reverse primer not found  
}
# To check the presence of primers after removal. Should appear as all 0.
primer_test <- rbind(FWD.ForwardReads = sapply(FWD.orients, primerHits, fn = fns.cut[[1]]),  
                         REV.ForwardReads = sapply(REV.orients, primerHits, fn = fns.cut[[1]]))
primer_test
fns <- fns.cut
```

## Summary after remove primers
```{r}
#SUmmary
summary <- data.frame()
fns.summary <- fns

    for(i in 1:length(fns.summary)) {
      geom <- fastq.geometry(fns.summary[i])
      summary_one_row <- data.frame (n_seq=geom[1], file_name=basename(fns.summary[i]))
      summary <- bind_rows(summary, summary_one_row)
      print(paste("Finished with file", fns.summary[i], ". ", round(i/length(fns.summary)*100, 2), "%", sep=""))
    }
    
write_tsv(summary, output_path( "_summary_after_cutadapt.tsv"))
```

```{r}
#Plot quality

for(i in 1:min(length(fns), max_plot_quality)) {
      print(str_c("i = ", i))
      p1 <- plotQualityProfile(fns[i])
      # if (i <= 2) {print(p1)}
      p1_file <- paste0(dir_qual, basename(fns[i]),".qual.pdf")
      ggsave( plot=p1, filename= p1_file, device = "pdf", width = 15, height = 15, scale=1, units="cm")
      
      read_length <- data.frame(length = width(ShortRead::readFastq(fns[i]))) # Read the fastQ file and get the length of all the reads...
      print(str_c("File before filtration", fns[i], "- Read length min=", min(read_length$length),"max=", max(read_length$length), "mean=", mean(read_length$length, na.rm=TRUE),  sep=" "))
      
      print(paste("Finished with file", fns[i], ". ", sep=""))
    }
```

## Filter and trim
```{r}
#Filter and trim - edit the parameters
  truncLen = c(230) # This influences the number of ASVs and the percent of asv recovered (need to remove 20 and 21). 
  minLen = c(220) #remove reads that are shorter.
  truncQ = 2         
  maxEE = c(10) #after truncation, reads with higher than expected errors are discarded. 
  
out <- filterAndTrim(fns, fns.filt, 
                           maxN=0, rm.phix=TRUE,
                           truncLen=truncLen[1], minLen=minLen[1], maxEE=maxEE[1],truncQ=truncQ,
                           compress=TRUE, multithread = multithread_filter)

print(out)
write_tsv(data.frame(out), output_path( "_summary_filtered_files.tsv")) 

```


## Generate error plots
```{r}
err_R1 <- learnErrors(fns.filt, multithread = multithread)
      p <- plotErrors(err_R1, nominalQ=TRUE)
      p_file <- output_path("_LearnErrors_R1.pdf")
      ggsave( plot=p, filename= p_file, device = "pdf", 
              width = 15, height = 15, scale=1, units="cm")
```

# DADA2
```{r}
derep_R1 <- derepFastq(fns.filt, n = 1e+05, verbose=T)
names(derep_R1) <- sample.names
dada_R1 <- dada(derep_R1, err=err_R1, multithread = multithread)
seqtab <- makeSequenceTable(dada_R1)
t_seqtab <- t(seqtab)
    
    # Only takes the first max_number_asvs rows
    if(exists("max_number_asvs")) {
      if(max_number_asvs > 0) {
        t_seqtab <- head(t_seqtab, max_number_asvs)
      }
    }
    
    print(table(nchar(getSequences(seqtab))))
    
    print(sprintf("Mean asv length : %.2f", mean(nchar(getSequences(seqtab)))))
```

## Remove chimeras
```{r}
 seqtab.nochim <- removeBimeraDenovo(seqtab, method=method_chimera, multithread=multithread, verbose=TRUE)
    
    p <- ggplot(data.frame(seq_length=nchar(getSequences(seqtab.nochim)))) +
      geom_histogram(aes(x=seq_length)) +
      ggtitle(str_c("Number of asv: ", ncol(seqtab.nochim)))
    p_file <- output_path("_asv_length_hist.pdf")
    ggsave( plot=p, filename= p_file, device = "pdf", 
            width = 15, height = 15, scale=1, units="cm") 
saveRDS(seqtab.nochim, output_path("_seqtab.nochim.rds")) 
```

```{r}
## Summary of reads at each step
getN <- function(x) sum(getUniques(x))
track <- cbind(out,
               sapply(dada_R1, getN), 
                     rowSums(seqtab), 
                     rowSums(seqtab.nochim))    
      colnames(track) <- c("intput", "filtered", "denoised", "tabled", "nonchim")
    track <- data.frame(track) %>% 
      mutate(file_code = sample.names)
    
    write_tsv(track, output_path("_summary_dada2.txt"))
```

```{r}
  #    Write fasta file without taxo
seqtab.nochim_trans <- as.data.frame(t(seqtab.nochim)) %>% 
      rownames_to_column(var = "sequence") %>%
      rowid_to_column(var = "asv_number") %>%
      mutate(asv_code = sprintf("asv_%03s_%05d", dataset_id, asv_number)) %>% 
      mutate(sequence = str_replace_all(sequence, "(-|\\.)",""))
    
    seq_out <- Biostrings::DNAStringSet(seqtab.nochim_trans$sequence)
    names(seq_out) <- seqtab.nochim_trans$asv_code
    Biostrings::writeXStringSet(seq_out, output_path("_no_taxo.fasta"), 
                                compress=FALSE, width = 20000)
```

# Assign taxonomy
```{r}
#seqtab.nochim <- readRDS("output_CTD_petB_normal_unpaired/TAN1810_CTD_normal_seqtab.nochim.rds")

#Assigning taxonomy
taxa_list <- list()
    
    n_asvs <- ncol(seqtab.nochim) 
    
    
    # Create the boundary of each slice
    slices = c(seq(from = 1, n_asvs, by=taxo_slice_size), n_asvs)
    #[1]  1  4  7 10 10
    
    # Remove the last slice if repeated
    slices <- unique(slices)
    # [1]  1  4  7 10
    
    for (i in 1:(length(slices)-1)){
      
      print(cat("Taxo slice = ", i, "\n"))
      
      seq_one <- seqtab.nochim[,slices[i]:slices[i+1]]
      
      taxa_one <- assignTaxonomy(seqs=seq_one,
                                 refFasta=database_path,
                                 taxLevels = tax_levels,
                                 minBoot = 0, outputBootstraps = TRUE,
                                 verbose = TRUE,
                                 multithread = multithread,
                                 tryRC = TRUE #only for R2 reads
                                 )
      boot_one <- data.frame(taxa_one$boot) %>%
        rename_all(funs(str_c(.,"_boot")))
      taxa_one <- data.frame(taxa_one$tax)  %>% 
        rownames_to_column(var = "sequence")
      
      taxa_list[[i]] <- bind_cols(taxa_one, boot_one)
    }
    
    taxa.df <- purrr::reduce(taxa_list, bind_rows)
    saveRDS(taxa.df, output_path("_taxa.rds"))
```

```{r}
# ASV table
    
    seqtab.nochim_trans <- as.data.frame(t(seqtab.nochim)) %>% 
      rownames_to_column(var = "sequence") %>%
      rowid_to_column(var = "asv_number") %>%
      mutate(asv_code = sprintf("asv_%03s_%05d", dataset_id, asv_number)) %>% 
      mutate(sequence = str_replace_all(sequence, "(-|\\.)","")) %>% 
      left_join(taxa.df) 
    
    write_tsv(seqtab.nochim_trans, output_path("_dada2.tsv"), na="")
    

# Create tables for import into database

    
    metapr2_asv <- seqtab.nochim_trans %>% 
      mutate(gene = gene, gene_region = gene_region, organelle = organelle, dataset_id = dataset_id) %>% 
      select(asv_code,sequence, asv_code:dataset_id)
    
    metapr2_asv$sequence_hash = purrr::map_chr(metapr2_asv$sequence,digest::sha1)
    
    write_tsv(metapr2_asv, output_path("_metapr2_asv.txt"), na="")
    
    metapr2_asv_abundance <- seqtab.nochim_trans %>% 
      select(-asv_number, -sequence, -(Domain:Subclade_boot)) %>% 
      gather("file_code", "n_reads", -contains("asv_code")) %>% 
      filter(n_reads > 0 )
    
    write_tsv(metapr2_asv_abundance, output_path("_metapr2_asv_abundance.txt"), na="")
```

# Make phyloseq RDS file
```{r}
library(phyloseq)
# Form OTU table
otus <- seqtab.nochim_trans %>% 
      select(-asv_number, -sequence, -(Domain:Subclade_boot)) %>%
  dplyr::distinct()
row.names(otus)<-otus$asv_code
otus <- otus %>%
  select(-asv_code)

# Form taxa table
taxa <- seqtab.nochim_trans %>%
  select(asv_code:Subclade) %>%
  dplyr::distinct()
row.names(taxa)<-taxa$asv_code
taxa <- taxa %>%
  select(-asv_code)

otus <- as.matrix(otus)
taxa <- as.matrix(taxa)

# Some samples were excluded so need to use semi_join to only include samples in the list.
#sample <- readxl::read_excel("sample_list/sampleList_CTD_petB_normal.xlsx")
sample # check sample data frame
sample_join <- semi_join(sample, sample_names_check, by = c("sample_name" = "sample.names"))
row.names(sample_join) <- sample_join$sample_name

otus= otu_table(otus, taxa_are_rows = TRUE)
taxa = tax_table(taxa)
sample_join = sample_data(sample_join, errorIfNULL = T)
row.names(sample_join) <- sample_join$sample_name

ps <- merge_phyloseq(otus, taxa, sample_join)

ps

saveRDS(ps, output_path("_phyloseq_asv_set.RDS"))

```

